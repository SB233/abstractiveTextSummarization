{
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "name": "",
  "signature": "sha256:1d3732c158cf449cf12d4934101323b432d5ac8376922b68f9b7a63d1867534a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# The Sequence to Sequence model\n",
      "\n",
      "A [Sequence to Sequence network](http://arxiv.org/abs/1409.3215), or seq2seq network, or [Encoder Decoder network](https://arxiv.org/pdf/1406.1078v3.pdf), is a model consisting of two separate RNNs called the **encoder** and **decoder**. The encoder reads an input sequence one item at a time, and outputs a vector at each step. The final output of the encoder is kept as the **context** vector. The decoder uses this context vector to produce a sequence of outputs one step at a time.\n",
      "\n",
      "![](https://i.imgur.com/tVtHhNp.png)\n",
      "\n",
      "When using a single RNN, there is a one-to-one relationship between inputs and outputs. We would quickly run into problems with different sequence orders and lengths that are common during translation. Consider the simple sentence \"Je ne suis pas le chat noir\" &rarr; \"I am not the black cat\". Many of the words have a pretty direct translation, like \"chat\" &rarr; \"cat\". However the differing grammars cause words to be in different orders, e.g. \"chat noir\" and \"black cat\". There is also the \"ne ... pas\" &rarr; \"not\" construction that makes the two sentences have different lengths.\n",
      "\n",
      "With the seq2seq model, by encoding many inputs into one vector, and decoding from one vector into many outputs, we are freed from the constraints of sequence order and length. The encoded sequence is represented by a single vector, a single point in some N dimensional space of sequences. In an ideal case, this point can be considered the \"meaning\" of the sequence.\n",
      "\n",
      "This idea can be extended beyond sequences. Image captioning tasks take an [image as input, and output a description](https://arxiv.org/abs/1411.4555) of the image (img2seq). Some image generation tasks take a [description as input and output a generated image](https://arxiv.org/abs/1511.02793) (seq2img). These models can be referred to more generally as \"encoder decoder\" networks."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Requirements\n",
      "\n",
      "You will need [PyTorch](http://pytorch.org/) to build and train the models, and [matplotlib](https://matplotlib.org/) for plotting training and visualizing attention outputs later."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import unicodedata\n",
      "import string\n",
      "import re\n",
      "import random\n",
      "import time\n",
      "import math\n",
      "import csv\n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.autograd import Variable\n",
      "from torch import optim\n",
      "import torch.nn.functional as F\n",
      "\n",
      "import os\n",
      "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
      "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6,7\"\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we will also define a constant to decide whether to use the GPU (with CUDA specifically) or the CPU. **If you don't have a GPU, set this to `False`**. Later when we create tensors, this variable will be used to decide whether we keep them on CPU or move them to GPU."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "USE_CUDA = False"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Loading data files\n",
      "\n",
      "The data for this project is a set of many thousands of English to French translation pairs.\n",
      "\n",
      "[This question on Open Data Stack Exchange](http://opendata.stackexchange.com/questions/3888/dataset-of-sentences-translated-into-many-languages) pointed me to the open translation site http://tatoeba.org/ which has downloads available at http://tatoeba.org/eng/downloads - and better yet, someone did the extra work of splitting language pairs into individual text files here: http://www.manythings.org/anki/\n",
      "\n",
      "The English to French pairs are too big to include in the repo, so download `fra-eng.zip`, extract the text file in there, and rename it to `data/eng-fra.txt` before continuing (for some reason the zipfile is named backwards). The file is a tab separated list of translation pairs:\n",
      "\n",
      "```\n",
      "I am cold.    Je suis froid.\n",
      "```\n",
      "\n",
      "For this summarization experiment, we adopt **DUC2003 dataset**, which is stored as the `/data/desc.txt` and `/data/head.txt` for input and target file. All the lines in these two files are seprerated by `\\n` and are in the same order between themselves."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Similar to the character encoding used in the character-level RNN tutorials, we will be representing each word in a language as a one-hot vector, or giant vector of zeros except for a single one (at the index of the word). Compared to the dozens of characters that might exist in a language, there are many many more words, so the encoding vector is much larger. We will however cheat a bit and trim the data to only use a few thousand words per language. On other tutorials,  they adopt an initial embedding matrix built from [GloVe](http://nlp.stanford.edu/projects/glove/), which can be tested afterwards."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Indexing words\n",
      "\n",
      "We'll need a unique index per word to use as the inputs and targets of the networks later. To keep track of all this we will use a helper class called `Voc` which has word &rarr; index (`word2index`) and index &rarr; word (`index2word`) dictionaries, as well as a count of each word `word2count` to use to later replace rare words."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "SOS_token = 0\n",
      "EOS_token = 1\n",
      "\n",
      "class Voc:\n",
      "    def __init__(self, name):\n",
      "        self.name = name\n",
      "        self.word2index = {}\n",
      "        self.word2count = {}\n",
      "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
      "        self.n_words = 2 # Count SOS and EOS\n",
      "      \n",
      "    def index_words(self, sentence):\n",
      "        for word in sentence.split(' '):\n",
      "            self.index_word(word)\n",
      "\n",
      "    def index_word(self, word):\n",
      "        if word not in self.word2index:\n",
      "            self.word2index[word] = self.n_words\n",
      "            self.word2count[word] = 1\n",
      "            self.index2word[self.n_words] = word\n",
      "            self.n_words += 1\n",
      "        else:\n",
      "            self.word2count[word] += 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Reading and decoding files\n",
      "\n",
      "The files are all in Unicode, to simplify we will turn Unicode characters to ASCII, make everything lowercase, and trim most punctuation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
      "def unicode_to_ascii(s):\n",
      "    return ''.join(\n",
      "        c for c in unicodedata.normalize('NFD', s)\n",
      "        if unicodedata.category(c) != 'Mn'\n",
      "    )\n",
      "\n",
      "# Lowercase, trim, and remove non-letter characters\n",
      "def normalize_string(s):\n",
      "    s = unicode_to_ascii(s.lower().strip())\n",
      "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
      "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
      "    return s"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To read the data file we will split the file into lines, and then split lines into pairs. The files are all description &rarr; headline, so if we want to generate text from headline &rarr; description I added the `reverse` flag to reverse the pairs."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def read_csv(csvname):\n",
      "    print(\"Reading csv...\")\n",
      "    \n",
      "    # Read the file and split into lines\n",
      "    with open('../data/%s.csv' % csvname, 'r') as csvfile:\n",
      "        sentences = []\n",
      "        for line in csvfile:\n",
      "            try:\n",
      "                sentences.append(normalize_string(line))\n",
      "            except IndexError:\n",
      "                pass\n",
      "\n",
      "    return sentences"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Filtering sentences\n",
      "\n",
      "Since there are a *lot* of example sentences and we want to train something quickly, we'll trim the data set to only relatively short and simple sentences. Here the maximum length is 50 words (that includes punctuation)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "MAX_LENGTH = 200\n",
      "\n",
      "def filter_sentences(sentences):\n",
      "    return [sentence for sentence in sentences if len(sentence) < MAX_LENGTH]\n",
      "\n",
      "# unnecessary here\n",
      "def filter_pair(p):\n",
      "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH \n",
      "\n",
      "def filter_pairs(pairs):\n",
      "    return [pair for pair in pairs if filter_pair(pair)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The full process for preparing the data is:\n",
      "\n",
      "* Read text file and split into lines, split lines into pairs\n",
      "* Normalize text, filter by length and content\n",
      "* Make word lists from sentences in pairs"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def prepare_csv_data(csvname, voc):\n",
      "    sentences = read_csv(csvname)\n",
      "    print(\"Read %s sentences\" % len(sentences))\n",
      "    \n",
      "    sentences = filter_sentences(sentences)\n",
      "    print(\"Trimmed to %s sentences\" % len(sentences))\n",
      "    \n",
      "    print(\"Indexing words...\")\n",
      "    for sentence in sentences:\n",
      "        voc.index_words(sentence)\n",
      "    \n",
      "    return sentences\n",
      "\n",
      "voc = Voc('source_identification')\n",
      "sentences_reddit = prepare_csv_data('reddit_sentence_list', voc)\n",
      "sentences_news = prepare_csv_data('news_sentence_list', voc)\n",
      "\n",
      "print(random.choice(sentences_reddit))\n",
      "print(random.choice(sentences_news))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Reading csv...\n",
        "Read 200 sentences\n",
        "Trimmed to 143 sentences\n",
        "Indexing words...\n",
        "Reading csv...\n",
        "Read 23 sentences\n",
        "Trimmed to 17 sentences\n",
        "Indexing words...\n",
        "definitely some of this going on . i think there s just a heightened sensitivity now too thanks internet !\n",
        " the announcement came as the us treasury imposed sanctions on iranian individuals and companies including the head of iran s judiciary .\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Turning training data into Tensors/Variables\n",
      "\n",
      "To train we need to turn the sentences into something the neural network can understand, which of course means numbers. Each sentence will be split into words and turned into a Tensor, where each word is replaced with the index (from the Lang indexes made earlier). While creating these tensors we will also append the EOS token to signal that the sentence is over.\n",
      "\n",
      "![](https://i.imgur.com/LzocpGH.png)\n",
      "\n",
      "A Tensor is a multi-dimensional array of numbers, defined with some type e.g. FloatTensor or LongTensor. In this case we'll be using LongTensor to represent an array of integer indexes.\n",
      "\n",
      "Trainable PyTorch modules take Variables as input, rather than plain Tensors. A Variable is basically a Tensor that is able to keep track of the graph state, which is what makes autograd (automatic calculation of backwards gradients) possible."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# Return a list of indexes, one for each word in the sentence\n",
      "def indexes_from_sentence(voc, sentence):\n",
      "    return [voc.word2index[word] for word in sentence.split(' ')]\n",
      "\n",
      "def variable_from_sentence(voc, sentence):\n",
      "    indexes = indexes_from_sentence(voc, sentence)\n",
      "    indexes.append(EOS_token)\n",
      "    var = Variable(torch.LongTensor(indexes).view(-1, 1))\n",
      "#     print('var =', var)\n",
      "    if USE_CUDA: var = var.cuda()\n",
      "    return var\n",
      "\n",
      "# unnecessary here\n",
      "def variables_from_pair(pair):\n",
      "    input_variable = variable_from_sentence(voc, pair[0])\n",
      "    target_variable = variable_from_sentence(voc, pair[1])\n",
      "    return (input_variable, target_variable)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Building the models"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## The Encoder\n",
      "\n",
      "<img src=\"images/encoder-network.png\" style=\"float: right\" />\n",
      "\n",
      "The encoder of a seq2seq network is a RNN that outputs some value for every word from the input sentence. For every input word the encoder outputs a vector and a hidden state, and uses the hidden state for the next input word."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "class EncoderRNN(nn.Module):\n",
      "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
      "        super(EncoderRNN, self).__init__()\n",
      "        \n",
      "        self.input_size = input_size\n",
      "        self.hidden_size = hidden_size\n",
      "        self.n_layers = n_layers\n",
      "        \n",
      "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
      "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
      "        \n",
      "    def forward(self, word_inputs, hidden):\n",
      "        # Note: we run this all at once (over the whole input sequence)\n",
      "        seq_len = len(word_inputs)\n",
      "        embedded = self.embedding(word_inputs).view(seq_len, 1, -1)\n",
      "        output, hidden = self.gru(embedded, hidden)\n",
      "        return output, hidden\n",
      "\n",
      "    def init_hidden(self):\n",
      "        hidden = Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n",
      "        if USE_CUDA: hidden = hidden.cuda()\n",
      "        return hidden"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Training\n",
      "\n",
      "## Defining a training iteration\n",
      "\n",
      "To train we first run the input sentence through the encoder word by word, and keep track of every output and the latest hidden state. Next the decoder is given the last hidden state of the decoder as its first hidden state, and the `<SOS>` token as its first input. From there we iterate to predict a next token from the decoder.\n",
      "\n",
      "### Teacher Forcing and Scheduled Sampling\n",
      "\n",
      "\"Teacher Forcing\", or maximum likelihood sampling, means using the real target outputs as each next input when training. The alternative is using the decoder's own guess as the next input. Using teacher forcing may cause the network to converge faster, but [when the trained network is exploited, it may exhibit instability](http://minds.jacobs-university.de/sites/default/files/uploads/papers/ESNTutorialRev.pdf).\n",
      "\n",
      "You can observe outputs of teacher-forced networks that read with coherent grammar but wander far from the correct translation - you could think of it as having learned how to listen to the teacher's instructions, without learning how to venture out on its own.\n",
      "\n",
      "The solution to the teacher-forcing \"problem\" is known as [Scheduled Sampling](https://arxiv.org/abs/1506.03099), which simply alternates between using the target values and predicted values when training. We will randomly choose to use teacher forcing with an if statement while training - sometimes we'll feed use real target as the input (ignoring the decoder's output), sometimes we'll use the decoder's output."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "teacher_forcing_ratio = 0.5\n",
      "clip = 5.0\n",
      "\n",
      "def train(input_variable, target_variable, encoder, encoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
      "\n",
      "    # Zero gradients of both optimizers\n",
      "    encoder_optimizer.zero_grad()\n",
      "    loss = 0 # Added onto for each word\n",
      "\n",
      "    # Get size of input and target sentences\n",
      "    input_length = input_variable.size()[0]\n",
      "\n",
      "    # Run words through encoder\n",
      "    encoder_hidden = encoder.init_hidden()\n",
      "    encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
      "    \n",
      "    loss = criterion(encoder_outputs[-1], target_variable)\n",
      "\n",
      "    loss.backward()\n",
      "    torch.nn.utils.clip_grad_norm(encoder.parameters(), clip)\n",
      "    encoder_optimizer.step()\n",
      "    \n",
      "    return loss.data[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally helper functions to print time elapsed and estimated time remaining, given the current time and progress."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def as_minutes(s):\n",
      "    m = math.floor(s / 60)\n",
      "    s -= m * 60\n",
      "    return '%dm %ds' % (m, s)\n",
      "\n",
      "def time_since(since, percent):\n",
      "    now = time.time()\n",
      "    s = now - since\n",
      "    es = s / (percent)\n",
      "    rs = es - s\n",
      "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Running training\n",
      "\n",
      "With everything in place we can actually initialize a network and start training.\n",
      "\n",
      "To start, we initialize models, optimizers, and a loss function (criterion)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "hidden_size = 500\n",
      "n_layers = 2\n",
      "dropout_p = 0.05\n",
      "\n",
      "# Initialize models\n",
      "encoder = EncoderRNN(voc.n_words, hidden_size, n_layers)\n",
      "\n",
      "# Move models to GPU\n",
      "if USE_CUDA:\n",
      "    encoder.cuda()\n",
      "\n",
      "# Initialize optimizers and criterion\n",
      "learning_rate = 0.0001\n",
      "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
      "criterion = nn.NLLLoss()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then set up variables for plotting and tracking progress:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# Configuring training\n",
      "n_epochs = 1000\n",
      "plot_every = 20\n",
      "print_every = 10\n",
      "\n",
      "# Keep track of time elapsed and running averages\n",
      "start = time.time()\n",
      "plot_losses = []\n",
      "print_loss_total = 0 # Reset every print_every\n",
      "plot_loss_total = 0 # Reset every plot_every"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To actually train, we call the train function many times, printing a summary as we go.\n",
      "\n",
      "*Note:* If you run this notebook you can train, interrupt the kernel, evaluate, and continue training later. You can comment out the lines above where the encoder and decoder are initialized (so they aren't reset) or simply run the notebook starting from the following cell."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Begin!\n",
      "for epoch in range(1, n_epochs + 1):\n",
      "    \n",
      "    # choose news or Reddit\n",
      "    source = random.randint(0, 1)\n",
      "    \n",
      "    # Get training data for this cycle\n",
      "    training_sentence = None\n",
      "    if source == 0:\n",
      "        training_sentence = variable_from_sentence(voc, random.choice(sentences_reddit))\n",
      "    else:\n",
      "        training_sentence = variable_from_sentence(voc, random.choice(sentences_news))\n",
      "\n",
      "    # Run the train function\n",
      "    loss = train(training_sentence, source, encoder, \n",
      "                 encoder_optimizer, \n",
      "                 criterion)\n",
      "\n",
      "    # Keep track of loss\n",
      "    print_loss_total += loss\n",
      "    plot_loss_total += loss\n",
      "\n",
      "    if epoch == 0: continue\n",
      "\n",
      "    if epoch % print_every == 0:\n",
      "        print_loss_avg = print_loss_total / print_every\n",
      "        print_loss_total = 0\n",
      "        print_summary = '%s (%d %d%%) %.4f' % (time_since(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg)\n",
      "        print(print_summary)\n",
      "\n",
      "    if epoch % plot_every == 0:\n",
      "        plot_loss_avg = plot_loss_total / plot_every\n",
      "        plot_losses.append(plot_loss_avg)\n",
      "        plot_loss_total = 0"
     ],
     "language": "python",
     "metadata": {
      "scrolled": false
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Plotting training loss\n",
      "\n",
      "Plotting is done with matplotlib, using the array `plot_losses` that was created while training."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.ticker as ticker\n",
      "import numpy as np\n",
      "%matplotlib inline\n",
      "\n",
      "def show_plot(points):\n",
      "    plt.figure()\n",
      "    fig, ax = plt.subplots()\n",
      "    loc = ticker.MultipleLocator(base=0.2) # put ticks at regular intervals\n",
      "    ax.yaxis.set_major_locator(loc)\n",
      "    plt.plot(points)\n",
      "\n",
      "show_plot(plot_losses)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Evaluating the network\n",
      "\n",
      "Evaluation is mostly the same as training, but there are no targets. Instead we always feed the decoder's predictions back to itself. Every time it predicts a word, we add it to the output string. If it predicts the EOS token we stop there. We also store the decoder's attention outputs for each step to display later."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def evaluate(sentence, max_length=200):\n",
      "    input_variable = variable_from_sentence(voc, sentence)\n",
      "    input_length = input_variable.size()[0]\n",
      "    \n",
      "    # Run through encoder\n",
      "    encoder_hidden = encoder.init_hidden()\n",
      "    encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
      "\n",
      "    # Create starting vectors for decoder\n",
      "    decoder_input = Variable(torch.LongTensor([[SOS_token]])) # SOS\n",
      "    decoder_context = Variable(torch.zeros(1, decoder.hidden_size))\n",
      "    if USE_CUDA:\n",
      "        decoder_input = decoder_input.cuda()\n",
      "        decoder_context = decoder_context.cuda()\n",
      "\n",
      "    decoder_hidden = encoder_hidden\n",
      "    \n",
      "    decoded_words = []\n",
      "    decoder_attentions = torch.zeros(max_length, max_length)\n",
      "    \n",
      "    # Run through decoder\n",
      "    for di in range(max_length):\n",
      "        decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
      "        decoder_attentions[di,:decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).cpu().data\n",
      "\n",
      "        # Choose top word from output\n",
      "        topv, topi = decoder_output.data.topk(1)\n",
      "        ni = topi[0][0]\n",
      "        if ni == EOS_token:\n",
      "            decoded_words.append('<EOS>')\n",
      "            break\n",
      "        else:\n",
      "            decoded_words.append(voc.index2word[ni])\n",
      "            \n",
      "        # Next input is chosen word\n",
      "        decoder_input = Variable(torch.LongTensor([[ni]]))\n",
      "        if USE_CUDA: decoder_input = decoder_input.cuda()\n",
      "    \n",
      "    return decoded_words, decoder_attentions[:di+1, :len(encoder_outputs)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can evaluate random sentences from the training set and print out the input, target, and output to make some subjective quality judgements:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def evaluate_randomly():\n",
      "    pair = random.choice(pairs)\n",
      "    \n",
      "    output_words, decoder_attn = evaluate(pair[0])\n",
      "    output_sentence = ' '.join(output_words)\n",
      "    \n",
      "    print('>', pair[0])\n",
      "    print('=', pair[1])\n",
      "    print('<', output_sentence)\n",
      "    print('')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "evaluate_randomly()"
     ],
     "language": "python",
     "metadata": {
      "scrolled": false
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Visualizing attention\n",
      "\n",
      "A useful property of the attention mechanism is its highly interpretable outputs. Because it is used to weight specific encoder outputs of the input sequence, we can imagine looking where the network is focused most at each time step.\n",
      "\n",
      "You could simply run `plt.matshow(attentions)` to see attention output displayed as a matrix, with the columns being input steps and rows being output steps:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "output_words, attentions = evaluate(\"house speaker newt gingrich who orchestrated the republican revolution of recent years and is overseeing the impeachment inquiry into president clinton was driven from office friday by a party that swiftly turned on him after its unexpected losses in tuesday s midterm elections .\")\n",
      "plt.matshow(attentions.numpy())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For a better viewing experience we will do the extra work of adding axes and labels:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def show_attention(input_sentence, output_words, attentions):\n",
      "    # Set up figure with colorbar\n",
      "    fig = plt.figure()\n",
      "    ax = fig.add_subplot(111)\n",
      "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
      "    fig.colorbar(cax)\n",
      "\n",
      "    # Set up axes\n",
      "    ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)\n",
      "    ax.set_yticklabels([''] + output_words)\n",
      "\n",
      "    # Show label at every tick\n",
      "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
      "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
      "\n",
      "    plt.show()\n",
      "    plt.close()\n",
      "\n",
      "def evaluate_and_show_attention(input_sentence):\n",
      "    output_words, attentions = evaluate(input_sentence)\n",
      "    print('input =', input_sentence)\n",
      "    print('output =', ' '.join(output_words))\n",
      "    show_attention(input_sentence, output_words, attentions)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "evaluate_and_show_attention(\"a south korean lawmaker said friday communist north korea could be producing plutonium and could have more secret underground nuclear facilities than already feared .\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "evaluate_and_show_attention(\"egyptian president hosni mubarak met here sunday with syrian president hafez assad to try to defuse growing tension between syria and turkey .\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "evaluate_and_show_attention(\"police and soldiers on friday blocked off the street in front of a house where members of a terrorist gang are believed to have assembled the bomb that blew up the u .s . embassy killing people .\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "evaluate_and_show_attention(\"premier battled tuesday for any votes freed up from a split in a far left party but said he will resign if he loses a confidence vote expected later this week .\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "collapsed": true
     },
     "source": [
      "# To do\n",
      "\n",
      "* Try with a different dataset\n",
      "    * cnn/dailymail\n",
      "    * gigawords\n",
      "    * standford\n",
      "    * Human &rarr; Machine (e.g. IOT commands)\n",
      "    * Chat &rarr; Response\n",
      "    * Question &rarr; Answer\n",
      "* Replace the embedding pre-trained word embeddings such as word2vec or GloVe\n",
      "* Try with more layers, more hidden units, and more sentences. Compare the training time and results.\n",
      "* Try different RNN layers like lstm.\n",
      "* Add batch operation for GPU training\n",
      "* Add beam search on decoder side when dealing with long documents.\n",
      "* Control the Different output size\n",
      "* Dig out other tricks"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}